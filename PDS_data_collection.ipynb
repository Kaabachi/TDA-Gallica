{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1> Combining our logs data with metadata from Gallica</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re \n",
    "import shutil\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from xml.etree import ElementTree as ET\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Useful functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OAI request to Gallica\n",
    "def OAI(id):\n",
    "\n",
    "    OAI_BASEURL = 'https://gallica.bnf.fr/services/OAIRecord?ark='\n",
    "\n",
    "    url = \"\".join([OAI_BASEURL, id])\n",
    "\n",
    "    s = requests.get(url, stream=True)\n",
    "    soup = BeautifulSoup(s.content,\"lxml-xml\")\n",
    "\n",
    "    return soup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of gallica themes, useful to interpret results from Gallica API queries\n",
    "dewey_classification = {}\n",
    "f = open(\"dewey.txt\", \"r\",encoding='utf-8')\n",
    "for x in f:\n",
    "    try:\n",
    "        if(x[2:4]=='0 '):\n",
    "            dewey_classification[x[0:2]]= x[4:].rstrip()\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "index_to_themes=dewey_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "'''\n",
    "get themes, date, title and language from ARKs using caching, if a user opens the same document twice we do not do any API call\n",
    "\n",
    "Input: list of arks\n",
    "\n",
    "Output: list of document titles\n",
    "\n",
    "'''\n",
    "\n",
    "def get_theme_from_ark(l):\n",
    "    temp_theme = []\n",
    "    temp_date = []\n",
    "    temp_title = []\n",
    "    temp_language =[]\n",
    "\n",
    "    # regular expression to only catch fields containing theme\n",
    "    for ark in l:  \n",
    "        theme = ''\n",
    "        date =''\n",
    "        title =''\n",
    "        language = ''\n",
    "        # remembering that l is a list of list [[ark1],[ark2],[ark3]]\n",
    "        # if ark is not empty\n",
    "        if(len(ark)>0):\n",
    "            try:\n",
    "                oai_result = OAI(ark[0]) \n",
    "                if(oai_result != None ):                        \n",
    "                    res = oai_result.results.notice.record.header.find_all(\"setSpec\")\n",
    "                    for e in res:\n",
    "                        if \"theme\" in e.text:\n",
    "                            theme= e.text.split(':')[3][:2]\n",
    "                            theme = index_to_themes.get(theme)\n",
    "                    date = oai_result.find(\"date\").text  \n",
    "                    title = oai_result.find(\"title\").text\n",
    "                    language = oai_result.find(\"language\").text\n",
    "\n",
    "            except:\n",
    "                theme = ''\n",
    "                date = ''\n",
    "                title = ''\n",
    "                language = ''\n",
    "\n",
    "        temp_date.append(date)\n",
    "        temp_theme.append(theme)\n",
    "        temp_title.append(title)\n",
    "        temp_language.append(language)\n",
    "\n",
    "    \n",
    "    return pd.Series([temp_theme, temp_date, temp_title, temp_language])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Gather data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read sessions previously gathered \n",
    "sessions = pd.read_csv('SessionsApril2016from300to1000_clean.csv',engine='python',error_bad_lines=False)  \n",
    "sessions.Ark = sessions.Ark.apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ark</th>\n",
       "      <th>Date</th>\n",
       "      <th>Country</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[bpt6k8730899], [bpt6k9664572c], [bpt6k967442...</td>\n",
       "      <td>['12/Apr/2016:01:28:15 +0200', '12/Apr/2016:02...</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>Mexico City</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[bpt6k8630600z], [btv1b53118063c], [bpt6k9672...</td>\n",
       "      <td>['06/Apr/2016:15:38:02 +0200', '06/Apr/2016:15...</td>\n",
       "      <td>France</td>\n",
       "      <td>Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[btv1b69253712], [btv1b69253712], [btv1b69176...</td>\n",
       "      <td>['08/Apr/2016:10:30:43 +0200', '08/Apr/2016:11...</td>\n",
       "      <td>France</td>\n",
       "      <td>Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[bpt6k9664572c], [btv1b531180658], [bpt6k9668...</td>\n",
       "      <td>['09/Apr/2016:23:22:21 +0200', '09/Apr/2016:23...</td>\n",
       "      <td>France</td>\n",
       "      <td>Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[bpt6k122592m], [bpt6k122592m]]</td>\n",
       "      <td>['08/Apr/2016:21:26:02 +0200', '08/Apr/2016:21...</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104187</th>\n",
       "      <td>[[btv1b84472995], [btv1b84472995], [btv1b84472...</td>\n",
       "      <td>['09/Apr/2016:00:24:02 +0200', '09/Apr/2016:00...</td>\n",
       "      <td>Poland</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104188</th>\n",
       "      <td>[[bpt6k5762405d], [bpt6k29835d], [bpt6k29835d]...</td>\n",
       "      <td>['09/Apr/2016:00:54:28 +0200', '09/Apr/2016:00...</td>\n",
       "      <td>France</td>\n",
       "      <td>Vouzeron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104189</th>\n",
       "      <td>[[btv1b105359774], [bpt6k9672622g], [btv1b1053...</td>\n",
       "      <td>['09/Apr/2016:22:56:25 +0200', '09/Apr/2016:22...</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>Tizi Ouzou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104190</th>\n",
       "      <td>[[bpt6k111503c], [bpt6k111503c], [bpt6k111503c...</td>\n",
       "      <td>['07/Apr/2016:22:58:38 +0200', '07/Apr/2016:22...</td>\n",
       "      <td>France</td>\n",
       "      <td>Cubzac-les-Ponts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104191</th>\n",
       "      <td>[[bpt6k873087j], [bpt6k9671229p], [bpt6k965733...</td>\n",
       "      <td>['11/Apr/2016:14:27:58 +0200', '11/Apr/2016:14...</td>\n",
       "      <td>Norway</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>104192 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      Ark  \\\n",
       "0       [[bpt6k8730899], [bpt6k9664572c], [bpt6k967442...   \n",
       "1       [[bpt6k8630600z], [btv1b53118063c], [bpt6k9672...   \n",
       "2       [[btv1b69253712], [btv1b69253712], [btv1b69176...   \n",
       "3       [[bpt6k9664572c], [btv1b531180658], [bpt6k9668...   \n",
       "4                        [[bpt6k122592m], [bpt6k122592m]]   \n",
       "...                                                   ...   \n",
       "104187  [[btv1b84472995], [btv1b84472995], [btv1b84472...   \n",
       "104188  [[bpt6k5762405d], [bpt6k29835d], [bpt6k29835d]...   \n",
       "104189  [[btv1b105359774], [bpt6k9672622g], [btv1b1053...   \n",
       "104190  [[bpt6k111503c], [bpt6k111503c], [bpt6k111503c...   \n",
       "104191  [[bpt6k873087j], [bpt6k9671229p], [bpt6k965733...   \n",
       "\n",
       "                                                     Date  Country  \\\n",
       "0       ['12/Apr/2016:01:28:15 +0200', '12/Apr/2016:02...   Mexico   \n",
       "1       ['06/Apr/2016:15:38:02 +0200', '06/Apr/2016:15...   France   \n",
       "2       ['08/Apr/2016:10:30:43 +0200', '08/Apr/2016:11...   France   \n",
       "3       ['09/Apr/2016:23:22:21 +0200', '09/Apr/2016:23...   France   \n",
       "4       ['08/Apr/2016:21:26:02 +0200', '08/Apr/2016:21...    Egypt   \n",
       "...                                                   ...      ...   \n",
       "104187  ['09/Apr/2016:00:24:02 +0200', '09/Apr/2016:00...   Poland   \n",
       "104188  ['09/Apr/2016:00:54:28 +0200', '09/Apr/2016:00...   France   \n",
       "104189  ['09/Apr/2016:22:56:25 +0200', '09/Apr/2016:22...  Algeria   \n",
       "104190  ['07/Apr/2016:22:58:38 +0200', '07/Apr/2016:22...   France   \n",
       "104191  ['11/Apr/2016:14:27:58 +0200', '11/Apr/2016:14...   Norway   \n",
       "\n",
       "                    City  \n",
       "0            Mexico City  \n",
       "1                  Paris  \n",
       "2                  Paris  \n",
       "3                  Paris  \n",
       "4                    NaN  \n",
       "...                  ...  \n",
       "104187               NaN  \n",
       "104188          Vouzeron  \n",
       "104189        Tizi Ouzou  \n",
       "104190  Cubzac-les-Ponts  \n",
       "104191               NaN  \n",
       "\n",
       "[104192 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing subsequent ARKs and remove empty lists\n",
    "def remove_consecutive_duplicates(l):\n",
    "    return [v for i, v in enumerate(l) if (i == 0 or v != l[i-1]) and v!=[]]\n",
    "\n",
    "    \n",
    "sessions['Ark'] = sessions.apply(lambda x: remove_consecutive_duplicates(x['Ark']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31363"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get only sessions that have 3<len<50\n",
    "sessions[\"len\"]= sessions.apply(lambda x: len(x['Ark']),axis=1)\n",
    "sessions = sessions[(sessions[\"len\"]<50) & (sessions[\"len\"]>3)]\n",
    "len(sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our data into 3 chunks to reduce API calls\n",
    "first_chunk = sessions[0:10000]\n",
    "second_chunk = sessions[10000:20000]\n",
    "third_chunk = sessions[20000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data into csv for later use\n",
    "for i in range(0,len(first_chunk),200):\n",
    "    temp_1 = first_chunk[i:i+200].apply(lambda x: get_theme_from_ark(x['Ark']),axis = 1)\n",
    "    temp_1.to_csv('first_chunk_300_1000.csv', mode='a', header=False)\n",
    "    print(i+200)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
